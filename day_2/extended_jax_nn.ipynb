{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4386b87d",
   "metadata": {},
   "source": [
    "# Advanced Nonlinear Regression with JAX\n",
    "\n",
    "*Prepared for the Computational Economics Workshop at Hitotsubashi*\n",
    "\n",
    "Author: [John Stachurski](https://johnstachurski.net)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Our next task is a challenging nonlinear regression with neural networks using JAX and Optax.\n",
    "\n",
    "In this task we will face a far more complex function, which cannot be fitted without a significant number of parameters.\n",
    "\n",
    "More parameters means minimization over a higher-dimensional loss surface, which will force us to  work harder with our optimization procedure.\n",
    "\n",
    "Thus, our JAX and Optax code will be correspondingly more advanced.\n",
    "\n",
    "At the same time, you will recognize many of the same core ideas.\n",
    "\n",
    "We begin with the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, NamedTuple\n",
    "from functools import partial\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa5b9c",
   "metadata": {},
   "source": [
    "Let's check our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using JAX version: {jax.__version__}\")\n",
    "print(f\"Device: {jax.devices()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5ed8b",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "The default configuration will have around 21,000 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Data parameters\n",
    "    data_size = 4_000\n",
    "    train_ratio = 0.8\n",
    "    noise_scale = 0.25\n",
    "    # Model parameters\n",
    "    hidden_layers = [128, 128, 32]\n",
    "    activation = \"selu\"  # Options: \"relu\", \"selu\", \"tanh\", \"sigmoid\"\n",
    "    # Training parameters\n",
    "    batch_size = 128\n",
    "    epochs = 20_000\n",
    "    init_lr = 0.001\n",
    "    min_lr = 0.0001\n",
    "    warmup_steps = 100\n",
    "    decay_steps = 300\n",
    "    regularization_term = 1e-5\n",
    "    # Evaluation\n",
    "    eval_every = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974e79d",
   "metadata": {},
   "source": [
    "Here is the function we will try to recover from noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef727b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Function to be estimated.\n",
    "    \"\"\"\n",
    "    term1 = 2 * jnp.sin(3 * x) * jnp.cos(x/2)\n",
    "    term2 = 0.5 * x**2 * jnp.cos(5*x) / (1 + 0.1 * x**2)\n",
    "    term3 = 3 * jnp.exp(-0.2 * (x - 4)**2) * jnp.sin(10*x)\n",
    "    term4 = 1.5 * jnp.tanh(x/3) * jnp.sin(7*x)\n",
    "    term5 = 0.8 * jnp.log(jnp.abs(x) + 1) * jnp.cos(x**2 / 8)\n",
    "    term6 = jnp.where(x > 0, 2 * jnp.sin(3*x), -2 * jnp.sin(3*x))  # Discontinuity\n",
    "    return term1 + term2 + term3 + term4 + term5 + term6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5531f",
   "metadata": {},
   "source": [
    "As you can see, this function is quite complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b28063",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = jnp.linspace(-10.0, 10.0, 200)\n",
    "fig, ax = plt.subplots()\n",
    "y_true = f(x_grid)\n",
    "ax.plot(x_grid, y_true, \n",
    "         color='black', \n",
    "         linewidth=2, label='true function')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53088a55",
   "metadata": {},
   "source": [
    "We will use the following function to produce noisy observations of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4091d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(\n",
    "        key: jax.Array,\n",
    "        data_size: int = Config.data_size\n",
    "    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate synthetic nonlinear regression data.\n",
    "\n",
    "    \"\"\"\n",
    "    x_key, y_key = jax.random.split(key)\n",
    "    # x is generated uniformly\n",
    "    x = jax.random.uniform(\n",
    "            x_key, (data_size, 1), minval=-10.0, maxval=10.0\n",
    "        )\n",
    "    # y = f(x) + noise\n",
    "    σ =  Config.noise_scale\n",
    "    w = σ * jax.random.normal(y_key, shape=x.shape)\n",
    "    y = f(x) + w\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a6f7e",
   "metadata": {},
   "source": [
    "## Constructing a network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be226845",
   "metadata": {},
   "source": [
    "Previously we used a dictionary to store the weights and biases associated with a single layer.\n",
    "\n",
    "Here we will used a `NamedTuple`, which feels slightly more elegant in scientific work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerParams(NamedTuple):\n",
    "    \"\"\"\n",
    "    Stores parameters for one layer of the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    W: jnp.ndarray     # weights\n",
    "    b: jnp.ndarray     # biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622cea5d",
   "metadata": {},
   "source": [
    "The weights and biases in each layer will be initialized randomly.\n",
    "\n",
    "We use standard initialization procedures according to the specified activation function.\n",
    "\n",
    "The next function initializes parameters in a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f44b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer_params(\n",
    "        key: jax.Array, \n",
    "        in_dim: int, \n",
    "        out_dim: int,\n",
    "        activation_name: str = Config.activation\n",
    "    ) -> Tuple[LayerParams, jax.Array]:\n",
    "    \"\"\"\n",
    "    Initialize parameters for a single layer using appropriate initialization\n",
    "    based on the activation function.\n",
    "    \n",
    "    - He initialization for ReLU and its variants\n",
    "    - LeCun initialization for SELU\n",
    "    - Glorot/Xavier initialization for tanh and sigmoid\n",
    "\n",
    "    \"\"\"\n",
    "    key, w_key, b_key = jax.random.split(key, 3)\n",
    "    \n",
    "    # Choose initialization strategy based on activation function\n",
    "    if activation_name == \"selu\":\n",
    "        # LeCun initialization \n",
    "        s = jnp.sqrt(1.0 / in_dim)\n",
    "        W = jax.random.normal(w_key, (in_dim, out_dim)) * s\n",
    "        b = jnp.zeros((out_dim,))\n",
    "    elif activation_name in [\"tanh\", \"sigmoid\"]:\n",
    "        # Glorot/Xavier initialization\n",
    "        s = jnp.sqrt(6.0 / (in_dim + out_dim))\n",
    "        W = jax.random.uniform(w_key, (in_dim, out_dim), minval=-s, maxval=s)\n",
    "        b = jnp.zeros((out_dim,))\n",
    "    else:\n",
    "        # He initialization (default for ReLU and variants)\n",
    "        s = jnp.sqrt(2.0 / in_dim)\n",
    "        W = jax.random.normal(w_key, (in_dim, out_dim)) * s\n",
    "        b = jnp.zeros((out_dim,))\n",
    "    \n",
    "    return LayerParams(W=W, b=b), key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc5618",
   "metadata": {},
   "source": [
    "Here's a function that uses the preceding logic to construct a Pytree of suitably initialized network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4633fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network_params(\n",
    "        key: jax.Array, \n",
    "        layer_sizes: List[int],\n",
    "        activation_name: str = Config.activation\n",
    "    ) -> List[LayerParams]:\n",
    "    \"\"\"\n",
    "    Initialize all parameters for the network and store them as a list of\n",
    "    instances of LayerParams (a Pytree).\n",
    "\n",
    "    \"\"\"\n",
    "    θ = []\n",
    "    # For all layers but the last one\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        # Generate an instance of LayerParams corresponding to layer i\n",
    "        layer, key = init_layer_params(\n",
    "            key, \n",
    "            layer_sizes[i],      # in dimension for layer\n",
    "            layer_sizes[i + 1],  # out dimension for layer\n",
    "            activation_name\n",
    "        )\n",
    "        # And append it to the list the contains all network parameters.\n",
    "        θ.append(layer)\n",
    "        \n",
    "    return θ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9994bc70",
   "metadata": {},
   "source": [
    "Here's a jitted function that maps inputs to outputs for a given parameterization of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=['activation'])\n",
    "def forward(\n",
    "        θ: List[LayerParams], \n",
    "        x: jnp.ndarray, \n",
    "        activation: str = Config.activation\n",
    "    ) -> jnp.ndarray:\n",
    "\n",
    "    \"\"\"\n",
    "    Forward pass through the neural network.\n",
    "    \n",
    "    Args:\n",
    "        θ: network parameters\n",
    "        x: input data\n",
    "        activation: activation function name (static argument)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the activation function based on name\n",
    "    if activation == \"relu\":\n",
    "        σ = jax.nn.relu\n",
    "    elif activation == \"selu\":\n",
    "        σ = jax.nn.selu\n",
    "    elif activation == \"tanh\":\n",
    "        σ = jnp.tanh\n",
    "    elif activation == \"gelu\":\n",
    "        σ = jax.nn.gelu\n",
    "    elif activation == \"sigmoid\":\n",
    "        σ = jax.nn.sigmoid\n",
    "    elif activation == \"elu\":\n",
    "        σ = jax.nn.elu\n",
    "    else:\n",
    "        # Default to selu\n",
    "        σ = jax.nn.selu\n",
    "    \n",
    "    # Apply all layers except the last, with activation\n",
    "    for W, b in θ[:-1]:\n",
    "        x = σ(x @ W + b)\n",
    "    # Apply last layer without activation (for regression)\n",
    "    W, b = θ[-1]\n",
    "    output = x @ W + b\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c82643",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601dd0a",
   "metadata": {},
   "source": [
    "The next function calculates loss associated with a given prediction vector in terms of MSE, conditional on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=['activation'])\n",
    "def mse_loss(\n",
    "        params: List[LayerParams], \n",
    "        x: jnp.ndarray,\n",
    "        y: jnp.ndarray,\n",
    "        activation: str = \"relu\"\n",
    "    ) -> jnp.ndarray:\n",
    "\n",
    "    \"\"\"\n",
    "    Mean squared error loss function.\n",
    "\n",
    "    \"\"\"\n",
    "    y_pred = forward(params, x, activation=activation)\n",
    "    return jnp.mean((y_pred - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3733b",
   "metadata": {},
   "source": [
    "When we compute loss, we will use a small amount of regularization to help prevent us from overfitting the existing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=['activation'])\n",
    "def regularized_loss(\n",
    "        params: List[LayerParams], \n",
    "        x: jnp.ndarray, \n",
    "        y: jnp.ndarray, \n",
    "        activation: str = \"selu\",\n",
    "        λ: float = Config.regularization_term\n",
    "    ) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Loss function with L2 regularization.\n",
    "\n",
    "    \"\"\"\n",
    "    mse = mse_loss(params, x, y, activation=activation)\n",
    "    \n",
    "    # L2 regularization\n",
    "    l2_penalty = 0.0\n",
    "    for layer in params:\n",
    "        l2_penalty += jnp.sum(layer.W ** 2)\n",
    "    \n",
    "    return mse + λ * l2_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdada4b",
   "metadata": {},
   "source": [
    "## Training Components\n",
    "\n",
    "In this section we implement some training components that execute key steps\n",
    "associated with updating the weights.\n",
    "\n",
    "First we write a function factory that performs a single update of the Pytree\n",
    "containing all parameters.\n",
    "\n",
    "The update uses Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step_factory(optimizer, activation: str = Config.activation):\n",
    "    \"\"\"\n",
    "    Create a JIT-compiled training step function.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a specialized loss gradient function for this activation\n",
    "    loss_grad = jax.grad(lambda p, x, y: regularized_loss(p, x, y, activation=activation))\n",
    "    \n",
    "    @jax.jit\n",
    "    def train_step(θ, opt_state, x_batch, y_batch):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        grads = loss_grad(θ, x_batch, y_batch)\n",
    "        loss_val = regularized_loss(θ, x_batch, y_batch, activation=activation)\n",
    "        \n",
    "        updates, new_opt_state = optimizer.update(grads, opt_state, θ)\n",
    "        θ = optax.apply_updates(θ, updates)\n",
    "        \n",
    "        return θ, new_opt_state, loss_val\n",
    "    \n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528eda0a",
   "metadata": {},
   "source": [
    "Now we create an Optax learning rate schedule with warmup and decay.\n",
    "\n",
    "The role of the schedule is to adjust the learning rate as training progresses.\n",
    "\n",
    "For details we refer to the Optax documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74005fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lr_schedule():\n",
    "    warmup_fn = optax.linear_schedule(\n",
    "        init_value=0.0,\n",
    "        end_value=Config.init_lr,\n",
    "        transition_steps=Config.warmup_steps\n",
    "    )\n",
    "    \n",
    "    decay_fn = optax.exponential_decay(\n",
    "        init_value=Config.init_lr,\n",
    "        transition_steps=Config.decay_steps,\n",
    "        decay_rate=0.5,\n",
    "        end_value=Config.min_lr\n",
    "    )\n",
    "    \n",
    "    return optax.join_schedules(\n",
    "        schedules=[warmup_fn, decay_fn],\n",
    "        boundaries=[Config.warmup_steps]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686555b",
   "metadata": {},
   "source": [
    "We also produce a data batch iterator, which generates a list containing data\n",
    "batches.  \n",
    "\n",
    "Each data batch is a subset of the data set containing matched input-output\n",
    "pairs.\n",
    "\n",
    "The collection of batches in the list can be understood as a random partition of\n",
    "the data set, where each element of the partition has the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_batch_iterator(\n",
    "        x: jnp.ndarray, \n",
    "        y: jnp.ndarray, \n",
    "        key: jax.Array,\n",
    "        batch_size: int,\n",
    "    ) -> List[Tuple[jax.Array]]:\n",
    "    \"\"\"\n",
    "    Create a list of batched data.  Each element of the list is a tuple\n",
    "    (x_batch, y_batch), containing a batch of data for training.\n",
    "\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Shuffle the data\n",
    "    indices = jax.random.permutation(key, jnp.arange(num_samples))\n",
    "    \n",
    "    # Create batches\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size  # Ceiling division\n",
    "    \n",
    "    batches = []\n",
    "    for i in range(num_batches):\n",
    "        batch_indices = indices[i * batch_size:(i + 1) * batch_size]\n",
    "        x_batch = x[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        batches.append((x_batch, y_batch))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef9bd43",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We are ready to train the network.\n",
    "\n",
    "First we set the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec24b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 # Set random seed for reproducibility\n",
    "key = jax.random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d5604",
   "metadata": {},
   "source": [
    "Now we produce separate keys for training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c8c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, train_data_key, val_data_key = jax.random.split(key, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bea382",
   "metadata": {},
   "source": [
    "Next we generate training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating data...\")\n",
    "train_data_size = Config.data_size\n",
    "x_train, y_train = generate_data(train_data_key, train_data_size)\n",
    "val_data_size = int(Config.data_size * 0.5)  # half of training data size\n",
    "x_val, y_val = generate_data(val_data_key, val_data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d018f",
   "metadata": {},
   "source": [
    "We also define model architecture and activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1  # scalar input\n",
    "output_dim = 1 # scalar output\n",
    "layer_sizes = [input_dim] + Config.hidden_layers + [output_dim]\n",
    "activation = Config.activation\n",
    "print(f\"Using activation function: {activation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2660b2",
   "metadata": {},
   "source": [
    "Let's initialize all the parameters in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b96cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initializing model with layer sizes: {layer_sizes}\")\n",
    "key, subkey = jax.random.split(key)\n",
    "θ = initialize_network_params(subkey, layer_sizes, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19742d04",
   "metadata": {},
   "source": [
    "Now let's train the network.\n",
    "\n",
    "Note that we are training a relatively large network and hence the training\n",
    "process takes a nontrivial amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer with learning rate schedule\n",
    "lr_schedule = create_lr_schedule()\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Gradient clipping for stability\n",
    "    optax.adam(learning_rate=lr_schedule)\n",
    ")\n",
    "opt_state = optimizer.init(θ)\n",
    "\n",
    "# Create training step function\n",
    "train_step_fn = training_step_factory(optimizer, activation)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "best_params = θ\n",
    "patience_counter = 0\n",
    "patience = 50   # Early stopping patience (in terms of evaluation intervals)\n",
    "\n",
    "print(f\"Starting training for {Config.epochs} epochs...\")\n",
    "start = time()\n",
    "\n",
    "# One epoch is a complete pass through the data set\n",
    "for epoch in range(Config.epochs):\n",
    "\n",
    "    # Create shuffled batches for this epoch\n",
    "    key, subkey = jax.random.split(key)\n",
    "    batches = create_data_batch_iterator(x_train, y_train, subkey, Config.batch_size)\n",
    "    \n",
    "    # Process each batch, updating parameters \n",
    "    epoch_losses = []\n",
    "    for x_batch, y_batch in batches:\n",
    "        θ, opt_state, loss = train_step_fn(θ, opt_state, x_batch,\n",
    "                                                y_batch)\n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "    # Calculate average loss for this epoch\n",
    "    avg_train_loss = jnp.mean(jnp.array(epoch_losses))\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Evaluate on validation set periodically\n",
    "    if epoch % Config.eval_every == 0 or epoch == Config.epochs - 1:\n",
    "\n",
    "        val_loss = float(mse_loss(θ, x_val, y_val, activation))\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_theta = jax.tree.map(lambda p: p, θ)  # Copy the params\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "elapsed = time() - start\n",
    "\n",
    "print(f\"Training completed in {elapsed:.2f} seconds.\")\n",
    "print(f\"Best validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cc30f",
   "metadata": {},
   "source": [
    "Here we plot the MSE curves on training and validation data over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10933cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_losses, label='training Loss')\n",
    "ax.plot(np.arange(0, len(val_losses) * Config.eval_every, Config.eval_every), \n",
    "             val_losses, label='validation Loss')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title(f'Learning curves with {Config.activation.upper()}')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa827b",
   "metadata": {},
   "source": [
    "Finally, let's plot the original and fitted functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00850118",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = jnp.linspace(-10.0, 10.0, 200)\n",
    "y_pred = forward(θ, x_grid.reshape(-1, 1), activation=activation)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# Plot training data\n",
    "ax.scatter(x_train.flatten(), y_train.flatten(), \n",
    "            alpha=0.2, color='blue', label='training data')\n",
    "\n",
    "# Plot the predicted curve\n",
    "ax.plot(x_grid, y_pred.flatten(), \n",
    "         color='red', \n",
    "         linewidth=2, \n",
    "         linestyle='--',\n",
    "         label='model prediction')\n",
    "\n",
    "# Plot the true function (without noise)\n",
    "y_true = f(x_grid)\n",
    "ax.plot(x_grid, y_true, \n",
    "         color='black', ls='--',\n",
    "         linewidth=2, label='true function')\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
