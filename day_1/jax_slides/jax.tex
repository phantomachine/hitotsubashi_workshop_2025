\input{../../preamble}

\title{Google JAX}
\subtitle{Prepared for the Computational Economics Workshop at Hitotsubashi}

\author{John Stachurski}


\date{2025}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\begin{frame}{Code}

    In this lecture series we will code in Python

    \pause

        \vspace{0.5em}
        \vspace{0.5em}
    Favorite libraries

    \begin{itemize}
        \item \brown{Google JAX}
        \vspace{0.2em}
        \item \brown{Google JAX}
        \vspace{0.2em}
        \item \brown{Google JAX}\ldots
        \vspace{0.2em}
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Topics}

    \begin{itemize}
        \item Scientific computing: history and background
        \vspace{0.5em}
        \item JIT compilation
        \vspace{0.5em}
        \item Autodiff
        \vspace{0.5em}
        \item Array operations
        \vspace{0.5em}
        \item Functional programming
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{History: Setting the stage}

    Before we can understand JAX, we need to know a bit about the history of scientific
    computing

    \vspace{0.5em}
    Let's recall some of the major paradigms and ideas:

    \vspace{0.5em}
    \begin{itemize}
        \item Languages and compilers
    \vspace{0.5em}
        \item Dynamic and static types
    \vspace{0.5em}
        \item Background on vectorization / JIT compilers
    \end{itemize}

\end{frame}
    


\begin{frame}
    \frametitle{Fortran / C  --- static types and AOT compilers}


    \Eg Suppose we want to compute the sequence
    %
    \begin{equation*}
        k_{t+1} = s k_t^\alpha + (1 - \delta) k_t
    \end{equation*}
    %
    from some given $k_0$ 

        \vspace{0.5em}
        \vspace{0.5em}
        \vspace{0.5em}

    Let's write a function in C that 
    %
    \begin{enumerate}
        \item implements the loop 
        \vspace{0.5em}
        \item returns the last $k_t$
    \end{enumerate}


\end{frame}


\begin{frame}[fragile]
    
    \begin{minted}{c}

int main() {
    double k = 0.2;
    double alpha = 0.4;
    double s = 0.3;
    double delta = 0.1;
    int i;
    int n = 1000;
    for (i = 0; i < n; i++) {
        k = s * pow(k, alpha) + (1 - delta) * k;
    }
    printf("k = %f\n", k);
}
    \end{minted}

\end{frame}



\begin{frame}[fragile]


    First we compile the whole program (ahead-of-time compilation):
    
    \vspace{0.5em}
    \begin{minted}{zsh}
❯❯ gcc solow.c -o out -lm
    \end{minted}



    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    Now we execute:

    \vspace{0.5em}
    \begin{minted}{zsh}
❯❯ ./out 
x = 6.240251
    \end{minted}

\end{frame}


\begin{frame}

    Pros

    \begin{itemize}
        \item fast arithmetic
        \item fast loops
    \end{itemize}


    \vspace{0.5em}

    Cons

    \begin{itemize}
        \item slow to write
        \item lack of portability
        \item hard to debug
        \item hard to parallelize
        \item low interactivity
    \end{itemize}

\end{frame}

\begin{frame}[fragile]


    For comparison, the same operation in Python:
    
    \begin{minted}{python}

α = 0.4
s = 0.3
δ = 0.1
n = 1_000
k = 0.2

for i in range(n-1):
    k = s * k**α + (1 - δ) * k

print(k)

    \end{minted}

\end{frame}


\begin{frame}
    
    Python is \emp{interpreted} rather than compiled

    \vspace{0.5em}
    \begin{itemize}
        \item code is executed statement by statement
    \vspace{0.5em}
        \item data types are queried on the fly
    \vspace{0.5em}
        \item arithmetic operations require method resolution
    \end{itemize}

\end{frame}

\begin{frame}

    Pros

    \begin{itemize}
        \item easy to write
        \item high portability
        \item immediate feedback --- high interactivity
        \item easy to debug
    \end{itemize}

    \vspace{0.5em}

    Cons

    \begin{itemize}
        \item slow
    \end{itemize}

\end{frame}


\begin{frame}
    
    So how can we get 

    \begin{center}
    good execution speeds \navy{and} high productivity / interactivity?
    \end{center}

\end{frame}

\begin{frame}
    \frametitle{Python + NumPy}

    \begin{figure}
       \begin{center} % l b r t
        \scalebox{.6}{\includegraphics[trim={2cm 8cm 6cm 3cm},clip]{numpy.pdf}}
       \end{center}
    \end{figure}

\end{frame}

\begin{frame}[fragile]

    \begin{minted}{python}
        import numpy 

        A = ((2.0, -1.0),
             (5.0, -0.5))

        b = (0.5, 1.0)

        A, b = np.array(A), np.array(b)

        x = np.inv(A) @ b
    \end{minted}

\end{frame}

\begin{frame}
    
    \begin{enumerate}
        \item Arrays defined with high-level commands 
        \vspace{0.5em}
        %
        \begin{itemize}
            \item (Python / NumPy API)
        \end{itemize}
        %
        \vspace{0.5em}
        \item Execution takes place in an efficient low-level environment
        \vspace{0.5em}
        %
        \begin{itemize}
            \item Efficient machine code (compiled C / Fortran)
        \end{itemize}
        %
        \vspace{0.5em}
        \item Results are returned to the high-level interface
    \end{enumerate}

\end{frame}


\begin{frame}
    \frametitle{MATLAB}

    NumPy is similar to and borrows from the older MATLAB programming
    environment

    \vspace{0.5em}
    \vspace{0.5em}
    \begin{figure}
       \begin{center} % l b r t
        \scalebox{.6}{\includegraphics[trim={2cm 8cm 6cm 3cm},clip]{matlab.pdf}}
       \end{center}
    \end{figure}


\end{frame}


\begin{frame}[fragile]
    
    \begin{minted}{matlab}
        A = [2.0, -1.0
             5.0, -0.5];

        b = [0.5, 1.0]';

        x = inv(A) * b
    \end{minted}
    
\end{frame}



    

\begin{frame}
    
    Advantages of NumPy / MATLAB

    \vspace{0.5em}
    \begin{itemize}
        \item Operations are passed to specialized machine code 
        \vspace{0.5em}
        \item Type-checking is paid per array, not per array element
    \end{itemize}

    \vspace{0.5em}
    \vspace{0.5em}
    Disadvantages 

    \begin{itemize}
        \item Can be highly memory intensive (intermediate arrays)
        \vspace{0.5em}
        \item \underline{Fails} to specialize on array \emp{shapes}
        \vspace{0.5em}
        \item Limited --- how would you accelerate the Solow code using NumPy?
    \end{itemize}

\end{frame}







\begin{frame}[fragile]
    \frametitle{Julia --- rise of the JIT compilers}

    Can do MATLAB / NumPy style vectorized operations

    \begin{minted}{julia}
A = [2.0  -1.0
     5.0  -0.5]

b = [0.5  1.0]'

x = inv(A) * b
    \end{minted}
    

    \vspace{0.5em}
    \vspace{0.5em}
    But also has fast loops via an efficient JIT compiler

\end{frame}


\begin{frame}
    
    \Eg Suppose, again, that we want to compute 
    %
    \begin{equation*}
        k_{t+1} = s k_t^\alpha + (1 - \delta) k_t
    \end{equation*}
    %
    from some given $k_0$ 


    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    \begin{itemize}
        \item Iterative, not easily vectorized
    \end{itemize}

\end{frame}


\begin{frame}[fragile]
    
    \begin{minted}{julia}

function solow(k0, α=0.4, δ=0.1, n=1_000)
    k = k0
    for i in 1:(n-1)
        k = s * k^α + (1 - δ) * k
    end
    return k
end

solow(0.2)  # JIT-compiled at first call
    \end{minted}

    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}

    Julia accelerates \texttt{solow} at runtime via a JIT compiler

\end{frame}


\begin{frame}
    
    Pros:

    \begin{itemize}
        \item fast execution --- assuming correct type inference
        \vspace{0.2em}
        \item dynamically typed\ldots (but compiler wants type stability)
        \vspace{0.2em}
        \item close to the maths
    \end{itemize}

        \vspace{0.2em}
        \vspace{0.2em}
        \vspace{0.2em}
        \vspace{0.2em}
        \vspace{0.2em}
        \vspace{0.2em}
    Cons:

    \begin{itemize}
        \item Everything compiled might not be optimal 
        \vspace{0.2em}
        \begin{itemize}
            \item debugging is more challenging
            \vspace{0.2em}
            \item slow first runs
            \vspace{0.2em}
        \end{itemize}
        \item Package instability
        \vspace{0.2em}
        \item Repeated breaking changes
    \end{itemize}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Python + Numba --- same architecture, same speed}
    
    \begin{minted}{python}
from numba import jit

@jit(nopython=True)
def solow(k0, α=0.4, δ=0.1, n=1_000):
    k = k0
    for i in range(n-1):
        k = s * k**α + (1 - δ) * k
    return k

solow(0.2)
    \end{minted}


    Runs at same speed as Julia / C / Fortran

\end{frame}


\begin{frame}
    
    OK, let's talk about the next generation\ldots

\end{frame}


\begin{frame}

    \begin{figure}
       \centering
       \scalebox{0.4}{\includegraphics{logo.pdf}}
    \end{figure}
    
            \vspace{0.5em}

    \begin{center}
        \url{https://jax.readthedocs.io/en/latest/}
    \end{center}

\end{frame}

\begin{frame}
    
    A high-performance numerical computing library 

            \vspace{0.5em}
            \vspace{0.5em}

    \begin{itemize}
        \item Developed by \href{https://research.google/}{Google Research}
            (prev.\ Google Brain)
            \vspace{0.5em}
        \item NumPy-style API for array operations
            \vspace{0.5em}
        \item GPU/TPU acceleration 
            \vspace{0.5em}
        \item Automatic differentiation
            \vspace{0.5em}
        \item Math-centric library semantics
            \vspace{0.5em}
        \item Rising popularity among ML researchers
    \end{itemize}

            \vspace{0.5em}
            \vspace{0.5em}

    ``The JAX compiler aims to enable researchers to write Python programs\ldots
        that are automatically compiled and scaled to leverage accelerators and
        supercomputers''

\end{frame}

\begin{frame}
    
    \Eg AlphaFold3 is built with Google JAX

        \vspace{0.5em}

    \begin{figure}
       \begin{center} % l b r t
        \scalebox{.24}{\includegraphics{af.pdf}}
       \end{center}
    \end{figure}


\end{frame}

\begin{frame}
    
    \textbf{Highly accurate protein structure prediction with AlphaFold}

        \vspace{0.5em}
    John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
    Olaf Ronneberger, Kathryn Tunyasuvunakool,\ldots 

        \vspace{0.5em}
    \underline{Nature} Vol.\ 596 (2021)

    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    \begin{itemize}
        \item Citation count $= ~35$K
        \vspace{0.5em}
        \item Nobel Prize in Chemistry 2024
    \end{itemize}

\end{frame}


\begin{frame}

    ``The acronym JAX stands for \brown{Just After eXecution}''

    \begin{itemize}
        \item monitor function execution once and then compile
    \end{itemize} 

            \vspace{0.5em}
            \vspace{0.5em}
            \vspace{0.5em}

    Another acronym:

    \begin{itemize}
        \item \brown{J}ust-in-time compilation
            \vspace{0.5em}
        \item \brown{A}utomatic differentiation
            \vspace{0.5em}
        \item \brown{X}LA (accelerated linear algebra)
    \end{itemize}



\end{frame}

\begin{frame}[fragile]
    \frametitle{Familiar NumPy-style array API}

    \begin{minted}{python}
        import jax.numpy as jnp

        A = ((2.0, -1.0),
             (5.0, -0.5))

        b = (0.5, 1.0)

        A, b = jnp.array(A), jnp.array(b)

        x = jnp.inv(A) @ b
    \end{minted}

\end{frame}


\begin{frame}[fragile]
    \frametitle{Implicit JIT via the XLA pipeline}

    The sequence of actions for performing \texttt{jnp.inv(A)} are as follows:

    %
    \begin{enumerate}
        \item JAX identifies that it needs to invert a matrix \texttt{A} of
            specific data type and shape
        \vspace{0.5em}
        \item JAX passes this information to XLA in an intermediate
            representation
        \vspace{0.5em}
        \item XLA \brown{generates compiled code specialized to your hardware, the 
            data type and shape of the array}
        \vspace{0.5em}
        \item The code is executed on the device and the result is returned to
            the user
        \vspace{0.5em}
        \item The code is cached in memory for future use (when called again
            with the same specific dtype and shape)
    \end{enumerate}

\end{frame}


\begin{frame}[fragile]
    \frametitle{Explicit just-in-time compilation}

    We can also explicitly JIT compile JAX functions
    
    \begin{minted}{python}
@jax.jit
def f(x):
    term1 = 2 * jnp.sin(3 * x) * jnp.cos(x/2)
    term2 = 0.5 * x**2 * jnp.cos(5*x) / (1 + 0.1 * x**2)
    term3 = 3 * jnp.exp(-0.2 * (x - 4)**2) * jnp.sin(10*x)
    return term1 + term2 + term3 
    \end{minted}

    \vspace{0.5em}
    \vspace{0.5em}

    \begin{itemize}
        \item Compiles at first call (e.g., \texttt{result = f(x)})
        \item Compiler specializes on \brown{both} shape and data type
    \end{itemize}

\end{frame}



\begin{frame}
    
    Compiler tools for optimizing function operations:

    \begin{itemize}
        \item Operations combined into fused kernels for GPU/TPU
        \vspace{0.5em}
        \item Eliminate intermediate buffers / memory writes and reads
        \vspace{0.5em}
        \item Loop unrolling
        \vspace{0.5em}
        \item Specialized algorithms
        \vspace{0.5em}
        \item Memory layout optimization for multi-dimensional arrays
    \end{itemize}

\end{frame}

\begin{frame}

    Implicit and explicit JIT 

    \begin{figure}[h!]
        \centering
        \scalebox{0.45}{\input{compiler_diagram.tex}}
    \end{figure}
    
\end{frame}




\begin{frame}[fragile]
    \frametitle{Automatic differentiation}
    
    \vspace{-1em}
    \begin{minted}{python}
import jax.numpy as jnp
from jax import grad, jit

def f(θ, x):
  for W, b in θ:
    w = x @ W + b
    x = jnp.tanh(w)  
  return x

def loss(θ, x, y):
  return jnp.sum((y - f(θ, x))**2)

grad_loss = jit(grad(loss))  # Now use gradient descent 
    \end{minted}

\end{frame}



\begin{frame}{More features of JAX}

    Let's review some other features

        \vspace{0.5em}

    \begin{itemize}
        \item Functional programming
        \vspace{0.5em}
        \item PyTrees
    \end{itemize}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Functional Programming}
    
    JAX adopts a \underline{functional} programming style

    \vspace{0.5em}
    \begin{itemize}
        \item[] $\implies$ Functions are \emp{pure}
    \end{itemize}


    \begin{minted}{python}

def f(θ, x):
  for W, b in θ:
    w = W @ x + b
    x = jnp.tanh(w)  
  return x

def loss(θ, x, y):
  return jnp.sum((y - f(θ, x))**2)

    \end{minted}


\end{frame}

\begin{frame}
    
    Pure functions:

    \begin{enumerate}
        \item \emp{Deterministic}
        \vspace{0.5em}
        \item \emp{No side effects}
    \end{enumerate}


    Deterministic means

    \begin{itemize}
        \item Same input $\implies$ same output 
        \vspace{0.5em}
        \item Outputs do not depend on global state
    \end{itemize}


    No side effects

    \begin{itemize}
        \item Won't change global state
        \vspace{0.5em}
        \item Won't modify data passed to the function
        \vspace{0.5em}
        \item Typically use immutable data
    \end{itemize}

\end{frame}


\begin{frame}[fragile]

    A \brown{non-pure} function
    \vspace{0.5em}
    \vspace{0.5em}

    \begin{minted}{python}
tax_rate = 0.1 
prices = [10.0, 20.0] 

def add_tax(prices):
    for i, price in enumerate(prices):
        prices[i] = price * (1 + tax_rate)    
    print('Modified prices: ', prices)
    return prices
    \end{minted}

    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    Why is this not pure?
    
\end{frame}



\begin{frame}[fragile]

    A \brown{pure} function
    \vspace{0.5em}
    \vspace{0.5em}

    \begin{minted}{python}
tax_rate = 0.1 
prices = (10.0, 20.0) 

def add_tax_pure(prices, tax_rate):
    return [price * (1 + tax_rate) for price in prices]
    \end{minted}
    
\end{frame}


\begin{frame}

    General advantages:

    \begin{itemize}
        \item Helps testing: each function can operate in isolation
        \vspace{0.5em}
        \item Promotes deterministic behavior and hence reproducibility
        \vspace{0.5em}
        \item Prevents bugs that arise from mutating shared state
    \end{itemize}

\end{frame}



\begin{frame}
    
    Advantages for JAX:

     \begin{itemize}
        \item Data dependencies are explicit, which helps with optimizing complex computations 
        \vspace{0.5em}
         \item Pure functions are easier to differentiate (autodiff)
        \vspace{0.5em}
         \item Pure functions are easier to parallelize and optimize (don't
             depend on shared mutable state)
        \vspace{0.5em}
         \item Transformations can be composed cleanly 
     \end{itemize}

        \vspace{0.5em}
        \vspace{0.5em}

     In summary, functional programming is good for 
     %
     \begin{itemize}
         \item JIT, autodiff, \& parallelization
     \end{itemize}


\end{frame}



\begin{frame}
    \frametitle{JAX PyTrees}

    Consider a function of the form
    %
    \begin{equation*}
        f_\theta
        = G_{m} \circ G_{m-1} \circ \cdots \circ G_{2}  \circ G_{1}
    \end{equation*}
    %
    where
    %
    \begin{itemize}
        \item $G_{\ell} x = \sigma_\ell(x W_\ell + b_\ell)$ 
            for $\ell = 1, \ldots, m$
        \vspace{0.5em}
        \item $\theta$ represents the ``vector'' of all parameters
        \vspace{0.5em}
        \item $\sigma_\ell$ is a given function
    \end{itemize}

        \vspace{0.5em}
        \vspace{0.5em}
    The idea that the vector $\theta$ contains all parameters is conceptually
    useful but awkward within code\ldots



\end{frame}


\begin{frame}

    To handle these kinds of situations we can use PyTrees
    
    \begin{itemize}
        \item A tree-like data structure built from Python containers
        \vspace{0.5em}
        \item A concept, not a data type
        \vspace{0.5em}
        \item Used to store parameters
    \end{itemize}

    \vspace{0.5em}
    \vspace{0.5em}
    \Egs

    \begin{itemize}
        \item A list of dictionaries, each dictionary contains parameters
        \item A dictionary of lists 
        \item A dictionary of lists of dictionaries
        \item etc.
    \end{itemize}

\end{frame}




\begin{frame}
    
    
    \resizebox{0.9\textwidth}{!}{
        \input{pytree_fig}
    }

\end{frame}


\begin{frame}
    
    JAX can

    \begin{itemize}
        \item apply functions to all leaves in a PyTree structure
        \vspace{0.5em}
        \item differentiate functions with respect to the leaves of PyTrees
        \vspace{0.5em}
        \item etc.
    \end{itemize}


\end{frame}

\begin{frame}
    

\begin{figure}
\centering
\begin{tikzpicture}[
  scale=0.5, transform shape, % Scale down the entire figure
  level distance=1.5cm,
  level 1/.style={sibling distance=2.5cm},
  level 2/.style={sibling distance=2cm},
  every node/.style={draw, rounded corners, fill=white, drop shadow, align=center, font=\sffamily},
  edge from parent/.style={thick, draw, -{Stealth[length=6pt]}},
  pytree/.style={draw, rounded corners, fill=blue!10, minimum width=1.8cm, minimum height=0.9cm},
  pytree2/.style={draw, rounded corners, fill=green!10, minimum width=1.8cm, minimum height=0.9cm},
  result/.style={draw, rounded corners, fill=purple!10, minimum width=1.8cm, minimum height=0.9cm},
  operation/.style={draw, circle, fill=red!20, inner sep=2pt}
]

% First pytree - unchanged
\node[pytree] (root1) at (-5,0) {pytree1};
\node[pytree, below left=1.5cm and 1.2cm of root1] (a1) {\texttt{'a'}: [1, 2, 3]};
\node[pytree, below right=1.5cm and 1.2cm of root1] (b1) {\texttt{'b'}: dict};
\node[pytree, below left=1.5cm and 0.4cm of b1] (c1) {\texttt{'c'}: [4, 5, 6]};
\node[pytree, below right=1.5cm and 0.4cm of b1] (d1) {\texttt{'d'}: 7};

\draw[edge from parent] (root1) -- (a1);
\draw[edge from parent] (root1) -- (b1);
\draw[edge from parent] (b1) -- (c1);
\draw[edge from parent] (b1) -- (d1);

% Second pytree - unchanged
\node[pytree2] (root2) at (5,0) {pytree2};
\node[pytree2, below left=1.5cm and 1.2cm of root2] (a2) {\texttt{'a'}: [10, 20, 30]};
\node[pytree2, below right=1.5cm and 1.2cm of root2] (b2) {\texttt{'b'}: dict};
\node[pytree2, below left=1.5cm and 0.4cm of b2] (c2) {\texttt{'c'}: [40, 50, 60]};
\node[pytree2, below right=1.5cm and 0.4cm of b2] (d2) {\texttt{'d'}: 70};

\draw[edge from parent] (root2) -- (a2);
\draw[edge from parent] (root2) -- (b2);
\draw[edge from parent] (b2) -- (c2);
\draw[edge from parent] (b2) -- (d2);


% Result pytree - shifted down from -6 to -7.5
\node[result] (rootR) at (0,-7.5) {result};
\node[result, below left=1.5cm and 1.2cm of rootR] (aR) {\texttt{'a'}: [11, 22, 33]};
\node[result, below right=1.5cm and 1.2cm of rootR] (bR) {\texttt{'b'}: dict};
\node[result, below left=1.5cm and 0.4cm of bR] (cR) {\texttt{'c'}: [44, 55, 66]};
\node[result, below right=1.5cm and 0.4cm of bR] (dR) {\texttt{'d'}: 77};

\draw[edge from parent] (rootR) -- (aR);
\draw[edge from parent] (rootR) -- (bR);
\draw[edge from parent] (bR) -- (cR);
\draw[edge from parent] (bR) -- (dR);

% Arrows from input pytrees to result - adjusted for new result position
%\draw[-{Stealth[length=6pt]}, thick, dashed, red] (root1) to[out=-30, in=150] (rootR);
%\draw[-{Stealth[length=6pt]}, thick, dashed, red] (root2) to[out=-150, in=30] (rootR);


\end{tikzpicture}
\caption{\texttt{jax.tree.map(lambda x, y: x + y, pytree1, pytree2)}}
\end{figure}

\end{frame}


\begin{frame}[fragile]
    
    \begin{minted}{python}
# Apply gradient updates to all parameters
def sgd_update(params, grads, learning_rate):
    return jax.tree.map(
        lambda p, g: p - learning_rate * g, 
        params, 
        grads
    )

# Calculate gradients (PyTree with same structure as params)
loss_grad = jax.grad(loss_fn)
grads = loss_grad(params, x, y)

# Update all parameters at once
updated_params = sgd_update(params, grads, 0.01)    
    \end{minted}

\end{frame}


\begin{frame}{Summary}
    
    Advantages over NumPy / MATLAB

    \vspace{0.5em}
    \begin{itemize}
        \item Machine code specialized to data types, shapes and devices!
        \vspace{0.5em}
        \vspace{0.5em}
        \item Automatically matches tasks with accelerators
        \vspace{0.5em}
        \vspace{0.5em}
        \item Same code, multiple backends (CPUs, GPUs, TPUs)
        \vspace{0.5em}
        \vspace{0.5em}
        \item Can fuse array operations for speed and memory efficiency
        \vspace{0.5em}
        \vspace{0.5em}
        \item Elegant functional style
        \vspace{0.5em}
        \vspace{0.5em}
        \item Integrated efficient autodiff
    \end{itemize}

\end{frame}

\begin{frame}

    Advantages of JAX (vs PyTorch / Tensorflow / etc.) for economists:
    %
    \begin{itemize}
        \item elegant functional programming style -- close to maths
            \vspace{0.5em}
        \item elegant autodiff tools
            \vspace{0.5em}
        \item array operations follow standard NumPy API
    \end{itemize}

            \vspace{0.5em}
            \vspace{0.5em}

    Exposes low level functions 

\end{frame}


\end{document}
